## Downloading the FASTQ-files from ArrayExpress
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR205/004/ERR2059824/ERR2059824.fastq.gz #sample1
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR205/005/ERR2059825/ERR2059825.fastq.gz #sample10
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR205/006/ERR2059826/ERR2059826.fastq.gz #sample11 
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR205/007/ERR2059827/ERR2059827.fastq.gz #sample12
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR205/008/ERR2059828/ERR2059828.fastq.gz #sample2
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR205/009/ERR2059829/ERR2059829.fastq.gz #sample3
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR205/000/ERR2059830/ERR2059830.fastq.gz #sample4
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR205/001/ERR2059831/ERR2059831.fastq.gz #sample5
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR205/002/ERR2059832/ERR2059832.fastq.gz #sample6
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR205/003/ERR2059833/ERR2059833.fastq.gz #sample7
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR205/004/ERR2059834/ERR2059834.fastq.gz #sample8
wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR205/005/ERR2059835/ERR2059835.fastq.gz #sample9

# Alternative way to download the files:
wget https://www.ebi.ac.uk/arrayexpress/files/E-MTAB-5935/E-MTAB-5935.sdrf.txt
cut -f26 E-MTAB-5935.sdrf.txt > column_with_links.txt
tail -n +2 column_with_links.txt > links2download.txt
wget -N -i links2download # Did not run this

#Unzip the files
gunzip *.fastq.gz

mkdir large_data
mv *.fastq ./large_data

# Download the genome of Mycobacterium tuberculosis from NCBI
# Get the files from https://www.ncbi.nlm.nih.gov/genome?LinkName=nuccore_genome&from_uid=444893469, 
# right click on the Download sequences in FASTA format for genome, protein -> genome, copy linkadress
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/195/955/GCF_000195955.2_ASM19595v2/GCF_000195955.2_ASM19595v2_genomic.fna.gz 
gunzip GCF_000195955.2_ASM19595v2_genomic.fna.gz
mv GCF_000195955.2_ASM19595v2_genomic.fna reference_sequence.fasta
mkdir data
mv reference_sequence.fasta ./data

## Aligning 

#Index the reference genome
bwa index ./data/reference_sequence.fasta

#Create folder for samfiles
mkdir results
mkdir ./results/samfiles

#Align sample 1
bwa mem ./data/reference_sequence.fasta ./large_data/ERR2059824.fastq > .results/samfiles/sample1.sam

#Align sample 10
bwa mem ./data/reference_sequence.fasta ./large_data/ERR2059825.fastq > ./results/samfiles/sample10.sam

#Align sample 11
bwa mem ./data/reference_sequence.fasta ./large_data/ERR2059826.fastq > ./results/samfiles/sample11.sam

#Align sample 12
bwa mem ./data/reference_sequence.fasta ./large_data/ERR2059827.fastq > ./results/samfiles/sample12.sam

#Align sample 2
bwa mem ./data/reference_sequence.fasta ./large_data/ERR2059828.fastq > ./results/samfiles/sample2.sam

#Align sample 3
bwa mem ./data/reference_sequence.fasta ./large_data/ERR2059829.fastq > ./results/samfiles/sample3.sam

#Align sample 4
bwa mem ./data/reference_sequence.fasta ./large_data/ERR2059830.fastq > ./results/samfiles/sample4.sam

#Align sample 5
bwa mem ./data/reference_sequence.fasta ./large_data/ERR2059831.fastq > ./results/samfiles/sample5.sam

#Align sample 6
bwa mem ./data/reference_sequence.fasta ./large_data/ERR2059832.fastq > ./results/samfiles/sample6.sam

#Align sample 7
bwa mem ./data/reference_sequence.fasta ./large_data/ERR2059833.fastq > ./results/samfiles/sample7.sam

#Align sample 8
bwa mem ./data/reference_sequence.fasta ./large_data/ERR2059834.fastq > ./results/samfiles/sample8.sam

#Align sample 9
bwa mem ./data/reference_sequence.fasta ./large_data/ERR2059835.fastq > ./results/samfiles/sample9.sam

# fast quality control of all samples
for i in {24..35}; do fastqc ./large_data/ERR20598$i.fastq; done
# could use this instead
fastqc *.fastq

# make a new directory
mkdir ./large_data/fastqc_files

# move files to own folder
mv *_fastqc* ./large_data/fastqc_files

#Convert samfiles into sorted bam files and bai files
cd ./results/samfiles
for file in *.sam; do samtools view -bS $file > ${file/.sam/.bam}; done
for file in *.bam; do samtools sort $file > ${file/.bam/.sorted.bam}; done
for file in *.sorted.bam do samtools index $file; done
cd ..

#SNP call using samtools mpileup 
for i in {1..12}; do samtools mpileup -g -f data/reference_sequence.fasta /results/samfiles/sample$i.sorted.bam > results/SNP_callng/sample$i_variants.bcf

# .bcf format into .vcf format
for i in {1..12}; do bcftools call -c -v ./results/SNP_calling/sample$i_variants.bcf > ./results/SNP_calling/${file/.bcf/.vcf}; done

# View .vfd file
less -S results/SNP_calling/sample1_variants.vcf

# Calculate average coverage for all samples
for i in {1..12}; do samtools depth ./results/samfiles/sample$i.sorted.bam | awk '{sum+=$3} END {print "Average = ",sum/4411532}'; done

#Filtering

#Filter reading depth to maximum 400
mkdir ./results/filtering
for i in {1..12}; do samtools depth ./results/samfiles/sample$i.sorted.bam | awk '$3<400' | cut -f 2 > ./results/filtering/sample$i.filtered_read_depths; done


cd results/filtering
mkdir sorted_qualityscore_vcf
cd ..

#Filter quality to minimum 10 
# sort with the quality score, start from line just above the numbers, 
# (not necessery to sort, but it is nice for a quick overlook of the scores)
for i in {1..12}; do tail -n +32 SNP_calling/sample${i}_variants.vcf |sort -n -k 6 > filtering/sorted_qualityscore_vcf/sample$i.sortedqs; done

cd filtering

# keep headline, and choose the ones with quality score >10
for i in {1..12}; do awk -F"\t" 'NR==1||$6>10' sorted_qualityscore_vcf/sample$i.sortedqs > sorted_qualityscore_vcf/sample$i.filtered_quality_scores_with_scores; done

# remove top line and cut the 2nd column with IDs
for i in {1..12}; do tail -n +2 sorted_qualityscore_vcf/sample$i.filtered_quality_scores_with_scores| cut -f 2 > sample$i.filtered_quality_scores; done


# Find common id:s from filtering on depth and quality score 
for i in {1..12}; do comm -12 <(sort sample$i.filtered_read_depths) <(sort sample$i.filtered_quality_scores) > sample$i.filtered_qs_depth; done

cd ..

mkdir SNP_unique_positions

#Extract positions that are unique in the DCS-resistant samples (samples 2-12), compared to the WT sample (sample 1)
for i in {2..12}; do comm -23 filtering/sample$i.filtered_qs_depth filtering/sample1.filtered_qs_depth > SNP_unique_positions/sample$i.unique.snp.pos; done

# Go back to Project home folder
cd ..





####TESTING ANOTHER FILTERING DUE TO UNSATISFYING RESULTS

mkdir results_depth600
#Filtering

#Filter reading depth to maximum 600
mkdir ./results/filtering600
for i in {1..12}; do samtools depth ./results/samfiles/sample$i.sorted.bam | awk '$3<600' | cut -f 2 > ./results_depth600/sample$i.filtered.positions; done
mkdir filtering
mv ./*.filtered.positions ./filtering

cp ./results/SNP_calling/*.vcf ./results_depth600
cd results_depth600
mkdir ./results_depth600/sorted_qualityscore_vcf
cd ./results_depth600

#Filter quality to minimum 10 
# sort with the quality score, start from line just above the numbers, 
# (not necessery to sort, but it is nice for a quick overlook of the scores)
for file in *.vcf; do tail -n +32 $file |sort -n -k 6 > ./sorted_qualityscore_vcf/${file/.vcf/.sortedqs}; done


cd sorted_qualityscore_vcf/
# keep headline, and choose the ones with quality score >10
for i in {1..12}; do awk -F"\t" 'NR==1||$6>10' sample$i* > sample$i.variants.removedqs; done

# Do not do this!!!
##### Now, sort the sample$i.variants.removedqs on ID. ***can change the sorting, see below at comm line
##### for i in {1..12};sort -n -k 2 sample$i.variants.removedqs > sample$i.variants.removedqs.sorted; done

# remove top line and cut the 2nd column with IDs
for i in {1..12}; do tail -n +2 sample$i.variants.removedqs| cut -f 2 > sample$i.variants.IDs; done

cd ..
mkdir filtering_qc_read_depth

# we sorted based on the wrong column in previous step, do a new sort to be able to compare the files. 
for i in {1..12}; do comm -12 <(sort filtering/sample$i.filtered.positions) <(sort sorted_qualityscore_vcf/sample$i.variants.IDs) > filtering_qc_read_depth/sample$i.final.IDs; done

#Extract positions for snps
mkdir SNP_positions 
for i in {1..12}; do cut -f 2 ./filtering_qc_read_depth/sample$i.final.IDs | sort > ./SNP_positions/sample$i.snp.pos; done

cd SNP_positions

#Extract positions that are unique in the DCS-resistant samples (samples 2-12), compared to the WT sample (sample 1)
for file in *.snp.pos; do if [ $file != sample1.snp.pos ]; then comm -23 $file sample1.snp.pos > ${file/.snp.pos/.unique.snp.pos}; fi; done
cd ..

# create new directory
mkdir SNP_results

# compare IDs from quality score + reading depth conditions with unique snp and put results in the new folder
for i in {2..12}; do comm -12 filtering_qc_read_depth/sample${i}.final.IDs SNP_positions/sample${i}.unique.snp.pos > SNP_results/sample${i}.snp; done

# Go back to Project home folder
cd ..


# In the article, they found xx unique SNPs. To see how many we found:
# Concatenate all SNPs for sample 2,â€¦,12. Then, sort them and filter out repeated lines.
# This will result in all unique SNPs 

for i in {2..12}; do cat results/SNP_unique_positions/sample$i.unique.snp.pos; done | sort | uniq > results/SNP_unique_positions/all_unique_SNP_positions

# Count rows to get number of unique SNPs

wc -l results/SNP_unique_positions/all_unique_SNP_positions





cd .






